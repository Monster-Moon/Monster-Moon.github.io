---
layout: post
title:  "Adaptive Stochastic Gradient Descent method."
date:   2018-02-15 12:00:00 +0100
categories:
---

## Adaptive Stochastic Gradient Descent method.

#
- Stochastic gradient 방법은 실시간으로 축적되는 데이터, 많은 양의 데이터에 사용하기 유용한 최적화 방법이다. 이 방법은 Adagrad, RMSprop, Adam 등의 새로운 방식으로 확장하기가 좋고 수렴성에도 좋은 성질을 가지고 있다.

- 그러나, 만약에 축적되는 데이터에 이상치가 포함되어 있다면? 혹은 생성 분포가 다른 두 개의 분포로 혼재되어 있다면?

- 이러한 경우 stochastic gradient가 갖는 불편성 (unbised)이 쉽게 무너지게 되고 편의를 가지게 되어 좋은 수렴성을 보장하기 어려울 것이다.

- 본 연구는 데이터가 두 개의 분포를 따른다고 가정하고 이러한 경우에 적용가능한 새로운 stochastic gradient 방법 제안하고자 한다. 

- 제안 알고리즘의 아이디어는 단순하다. 예를 들어, 두 분포를 A, B라고 하고 A에 해당하는 모수를 추정하기 위해서 테스트 과정을 추가하여 batch sample이 A 분포를 따른다면 업데이트를 하고, 아니라면 하지 않는 방식이다.

- 당연하게도, 제안 알고리즘의 수렴성은 분포를 따르는지 따르지 않는지에 대한 테스트의 성능에 의존할 것이고, 실제로 추정된 해의 수렴 반경 (convergence radius)가 테스트의 성능에 의존함을 보였다. 

- 또한, stochastic gradient 방법이 테스트에 관계없이 update를 한다는 점에 착안하여 사용자가 분포의 혼재를 인식하지 못하고 stochastic gradient 방법을 통해 최적화를 진행했을 때 추정되는 해의 성질을 보일 수 있었다.

- 논문 게재에 성공했다! <a href= 'https://www.dbpia.co.kr/pdf/pdfView.do?nodeId=NODE07404368&mark=0&useDate=&bookmarkCnt=0&ipRange=N&accessgl=Y&language=ko'>[link]</a>

